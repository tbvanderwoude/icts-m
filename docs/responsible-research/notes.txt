Regarding scientific integrity
Millikan, Ehrenhaft: worked on measurement of electrical charges, fundamental electronic charge etc.

The first was a cherry-picking fraud who won a Nobel prize -> but inclusion of the data-points left-out would have yielded the same conclusion. 
The second did not manipulate Data as far as is known but his subelectron-theory was wrong, which was the basis of his critique of Millikan.

Three classes of data-manipulation:
Fabrication; falsification; trimming

Trimming is not always bad: but it needs explanation and ideally a way to show the left-out data if requested (practically not always possible, consider PBs of data generated by particle accelerators). Having a log-book also helps for something not generating LHC-type volumes of data.

Diederik Stapel: starting in 2004 or even earlier, fabricated/falsified data for 55 publications, until 2011.


Professor Esveld: "My chair is largely paid by ProRail, so I cannot afford to be too critical of them"

No breach of scientific integrity according to TU Delft???

Conflicts of interest: not always morally wrong, but definitely improper.

Nepotism, owning of certain stocks in certain roles (climate chair and oil stocks and so on), gifts/bribes, outside employment, self-dealing.

Dealing with conflicts of interest:
Avoid immoral actions/fraud
Actively seperate roles (art of seperation)
Removal (sell stocks, leave secondary employment and so on)
Disclosure
Abstaining from certain decisions
Third-party evaluation

Plagiarism:
Presenting another's ideas (in the broadest sense) as your own.

Not just quotations! Sentence/argument structures, ideas, research results and so on.

Goal: allow for organized skepticism, writing where you can follow the trace of sources and information consulted (are the sources/their methods any good?) -> cite anything you should cite.
Also giving/receiving credit when it is due is important.

Self-plagiarism, 'salami slicing', rehashing of old work: it is a different beast.
Peter Nijkamp

Points of critique: lack of trial, repetition, sometimes copyright issues (contracts with publishers are often more-or-less exclusive, that is why copyright is worth money in publishing)

When are you an author? Perhaps you brought in ideas, contributed in writing or editing to a paper. At the very least, for a paper, you drafted and/or reviewed it.

Warning: being an author means that you are also willing to accept (part of the) blame


Experimenting with human subjects: always touchy, especially since WW2, Tuskegee siphilis study etc.

Crucial: informed consent, respecting human dignity, clear benefits (e.g. monetary) / risks (e.g. health)

Human Research Ethics Committee at TU Delft: reviews all research with human subjects in the loop.




Flint: switched from Lake Huron to water running through Flint.
E. Coli in water. Also: industrial chemicals, sewage, road salt...

Oh and there was lead in the water: tons of it.

Why?

No optimized corrosion control
Few samples taken at the wrong places
Throwing out of samples to stay under trigger-level for further action

Questionable pattern of assigning governor-appointed emergency managers to black-majority cities. One of which decided to use the river water.

The water was corroding GM's machine parts

What went wrong in sampling? There were 60 samples, although 100.000 residents means 100 samples (city had 99.000, so just a little under this mark). Residents pre-flushed before testing, lowering detections in samples. Samples were connected in small-neck bottles, lowering the speed of the stream and lead corrosion from pipes making it into stream as a consequence. Also, the most at-risk homes were to be tested but no inventory of this was available.


Iron level exceeded measurement capabilities, lead levels were more than 20 times too high sometimes.

Power of citizen scientists and emperical evidence, sadness of officials focussing only on showing regulations are met.


Reproducibility crisis! Most researchers think there is one, no one does something about it. A bit like climate change.

People fail to reproduce other's work, even their own work! 

Having other lab members redo experiment is a good sanity check. Documentation and standardization of procedures are also important.

Pre-registration: I will do this experiment with this analysis after to test these hypotheses. 

Factors: pressure to publish and selective reporting.

Also: increasing grant and position competition, more bureaucracy and additional tasks: being stretched thinner.

Reproducibility is like brushing your teeth

Diederik stapel



Stapel: opposite extreme of the subtle cherry-picking to support your hypothesis. Still on the same distribution, that of fraud

Science is not holy.

Science becoming a business
Resources, grants, competition, money

Travelling salesmen selling their talks, maybe more like a travelling circus

Lesson: there was space in the scientific praxis of the field of psychology for fraud at this scale, with so little care (handcrafted random data and so on).

Reproducibility in computational science:
Massive computation as changing science, many different fields.
Simulations from the climate to the genesis of galaxies and biomolecular processes. Data mining and artificial intelligence. 

Computation as key in modern scientific praxis: so experiments, code, should be described as accurately as traditional science but it is not!
This is the scientific method. In math, science, you need to explain what you found out and how you did it so people can check and see that it is true.

Why is it harder? Communicating the exact algorithms you used, what software packages and so on, is not a fit for a traditional paper. Code repositories do not always make sense. Data is often too huge to put up online, or may have copyrights. 

Credibility crisis

Fundamental question: how can data and code be integrated with traditional research publication formats?

Bermuda principles in genome science

Roles: scientist, funding decision-maker, journal editor

Scientist: Include source-code and data along with statistical analyses and simulation results.
Version this code properly, so that versions could be cited in reproduction (doi: digital object identifier)
Describe environment, ideally a virtual machine image
Use open licensing to facilitate reuse
Use open-access contracts for published papers
Avoid proprietary formats/systems/software

Funding role:
Establish archival organization for hosting data/code
Fund groups to adopt reproducibility methods
Encourage development of common definitions regarding state of reproducibility
Fund development of new digital frameworks/tools for both linking code/data to pubs but also for sharing workflows.

Journal editor:
Encourage provision of URLs to repos
Require a replication review
Require appropriate data/code citations when referring to data/code

Long-term:
VCS for data and code and papers. 
Test suites delivered along with code as well as documentation (See Sphinx)
Tools for describing contributions, even if small (minor commits to code)
Tools for effective download tracking
Marking of reproducible documents in a uniform way
Standard data descriptions
Universities taking a key role in all the above, to facilitate this
Clarify ownership between all parties of data, code, paper
Develop living communities around all these concepts related to computational reproducibility


